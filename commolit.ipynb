{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "commolit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN46XUuvEcxNKhRzQtERysK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mya-mori/Kaggle/blob/CoML/commolit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRS5iQvHf0K-"
      },
      "source": [
        "#ENGLISH\n",
        "#音節のカウント\n",
        "def syllable_count(word):\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "            if word.endswith(\"e\"):\n",
        "                count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "xtrain['col'] =  xtrain['word'].apply(lambda s: syllable_count(s))\n",
        "xtest['col'] =  xtest['word'].apply(lambda s: syllable_count(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDZdXFc7kxtF"
      },
      "source": [
        "# count the characters\n",
        "xtrain['col_ch'] = xtrain['word'].apply(len)\n",
        "xtest['col_ch'] = xtest['word'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-igkGwj4scWd"
      },
      "source": [
        "# count the words\n",
        "xtrain['col_word'] = xtrain['word'].apply(lambda s: len(s.split(' ')))\n",
        "xtest['col_word'] = xtest['word'].apply(lambda s: len(s.split(' ')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ItjsHDitE0j"
      },
      "source": [
        "# words to characters\n",
        "xtrain['w2c'] = xtrain['col_word'] / xtrain['col_ch']\n",
        "xtest['w2c'] = xtest['col_word'] / xtest['col_ch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWnWhRUy57Sv"
      },
      "source": [
        "# nof sentences\n",
        "xtrain['nof_sentences'] =  xtrain['word'].apply(lambda s: s.count('.'))\n",
        "xtest['nof_sentences'] =  xtest['word'].apply(lambda s: s.count('.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg7VYV726O-1"
      },
      "source": [
        "# nof syllables\n",
        "xtrain['nof_syllables'] =  xtrain['word'].apply(lambda s: syllable_count(s))\n",
        "xtest['nof_syllables'] =  xtest['word'].apply(lambda s: syllable_count(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrhqBRkuBytu"
      },
      "source": [
        "# count the unique words\n",
        "xtrain['nof_unique_words'] = xtrain['word'].apply(lambda s: len(set(s.split(' ') )))\n",
        "xtest['nof_unique_words'] = xtest['word'].apply(lambda s: len(set(s.split(' ') )))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s-OnhkzCDcy"
      },
      "source": [
        "# text diversity\n",
        "xtrain['txt_diversity'] = xtrain['nof_unique_words'] / xtrain['nof_words']\n",
        "xtest['txt_diversity'] = xtest['nof_unique_words'] / xtest['nof_words']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nFgFH8PCJ5B"
      },
      "source": [
        "# word lengths\n",
        "words = xtrain['excerpt'].apply(lambda s: s.split(' '))\n",
        "word_lengths = words.apply(lambda s: [len(f) for f in s ])\n",
        "xtrain['longest_word'] = word_lengths.apply(max)\n",
        "xtrain['avg_word'] = word_lengths.apply(np.mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxZQ2OxnCTX5"
      },
      "source": [
        "#wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='black',\n",
        "        stopwords=stopwords,\n",
        "        max_words=200,\n",
        "        max_font_size=40, \n",
        "        scale=3,\n",
        "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
        ").generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize=(15, 15))\n",
        "    plt.axis('off')\n",
        "    if title: \n",
        "        fig.suptitle(title, fontsize=20)\n",
        "        fig.subplots_adjust(top=2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()\n",
        "\n",
        "show_wordcloud(df['excerpt'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}